# Simple dict-based scorecard generator
import sys
sys.path.append('src')
import json
from pathlib import Path
import argparse

def generate_scorecards(analytics_file, out_dir, verbose=False):
    # Load data as dict
    with open(analytics_file, 'r') as f:
        data = json.load(f)
    
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)
    rulebooks_dir = out_path / "rulebooks"
    rulebooks_dir.mkdir(exist_ok=True)
    
    files_created = []
    
    for rb in data['rulebook_analytics']:
        rulebook_id = rb['identity']['rulebook_id']
        
        # Create simple scorecard
        scorecard = {
            'rulebook_id': rulebook_id,
            'source_pdf': rb['identity']['source_pdf_path'],
            'total_images': rb['extraction_outcome']['images_attempted'],
            'success_rate': rb['extraction_outcome']['success_rate'],
            'failure_rate': rb['extraction_outcome']['failure_rate'],
            'analytics_source': f"corpus_analytics.json -> rulebook_analytics[{rulebook_id}]"
        }
        
        # Write JSON
        json_file = rulebooks_dir / f"{rulebook_id}.json"
        with open(json_file, 'w') as f:
            json.dump(scorecard, f, indent=2)
        files_created.append(json_file)
        
        # Write simple markdown
        md_content = f"""# QA Scorecard: {rulebook_id}

**Source PDF:** {scorecard['source_pdf']}

## Summary Metrics

| Metric | Value |
|--------|-------|
| Total Images | {scorecard['total_images']:,} |
| Success Rate | {scorecard['success_rate']:.1%} |
| Failure Rate | {scorecard['failure_rate']:.1%} |

## Evidence Anchors

- **Analytics Source:** {scorecard['analytics_source']}

---
*Generated by Hephaestus QA Scorecard Generator*
"""
        
        md_file = rulebooks_dir / f"{rulebook_id}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        files_created.append(md_file)
    
    return files_created

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Phase 8 Decision-Grade QA: Rulebook Scorecards")
    parser.add_argument("--analytics", required=True, help="Path to analytics directory or corpus_analytics.json")
    parser.add_argument("--out", required=True, help="Output directory for QA scorecards")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    # Determine input file
    analytics_path = Path(args.analytics)
    if analytics_path.is_file() and analytics_path.name == "corpus_analytics.json":
        analytics_file = analytics_path
    elif analytics_path.is_dir():
        analytics_file = analytics_path / "corpus_analytics.json"
        if not analytics_file.exists():
            print(f"Error: corpus_analytics.json not found in {analytics_path}")
            sys.exit(1)
    else:
        print(f"Error: {analytics_path} must be a directory containing corpus_analytics.json or the file itself")
        sys.exit(1)
    
    if args.verbose:
        print(f"Loading analytics from: {analytics_file}")
    
    files = generate_scorecards(analytics_file, args.out, args.verbose)
    print(f"Generated {len(files)} scorecard files in {args.out}")
    
    if args.verbose:
        for f in files:
            print(f"  {f}")
